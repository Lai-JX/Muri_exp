/home/jxlai/project/Muri_exp/workloads
--scheduler-ip 127.0.0.1 --trainer-port 9013 --this-dir /home/jxlai/project/Muri_exp/workloads
/home/jxlai/project/Muri_exp/workloads/hostfiles/hostfile-[0--1--1--1]-[1-0-0-0]
8 4 --hostfile /home/jxlai/project/Muri_exp/workloads/hostfiles/hostfile-[0--1--1--1]-[1-0-0-0]
-------------------------------
vgg16 16 0 0 0 0 0 0
ssh: Could not resolve hostname worker-1: Temporary failure in name resolution
ssh: Could not resolve hostname worker-0: Temporary failure in name resolution
--------------------------------------------------------------------------
ORTE was unable to reliably start one or more daemons.
This usually is caused by:

* not finding the required libraries and/or binaries on
  one or more nodes. Please check your PATH and LD_LIBRARY_PATH
  settings, or configure OMPI with --enable-orterun-prefix-by-default

* lack of authority to execute on one or more specified nodes.
  Please verify your allocation and authorities.

* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).
  Please check with your sys admin to determine the correct location to use.

*  compilation of the orted with dynamic libraries when static are required
  (e.g., on Cray). Please check your configure cmd line and consider using
  one of the contrib/platform definitions for your system type.

* an inability to create a connection back to mpirun due to a
  lack of common network interfaces and/or no route found between
  them. Please check network connectivity (including firewalls
  and network routing requirements).
--------------------------------------------------------------------------
--------------------------------------------------------------------------
ORTE does not know how to route a message to the specified daemon
located on the indicated node:

  my node:   gpu1
  target node:  worker-0

This is usually an internal programming error that should be
reported to the developers. In the meantime, a workaround may
be to set the MCA param routed=direct on the command line or
in your environment. We apologize for the problem.
--------------------------------------------------------------------------
